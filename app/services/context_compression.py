"""Context compression service for managing conversation history.

This service handles the compression of conversation history when token limits
are approached, using a sliding window approach to maintain context while
reducing token usage.
"""

from typing import List, Optional

from app.llm import LLM
from app.logger import logger
from app.schema import Memory, Message


class ContextCompressionService:
    """Service for compressing conversation context when token limits are approached.

    Uses a sliding window approach:
    - Keeps all system messages
    - Keeps the most recent N messages (configurable)
    - Summarizes messages in between
    """

    # Compression thresholds
    MIN_MESSAGES_TO_COMPRESS = 5
    MAX_MESSAGES_BEFORE_COMPRESS = 20
    MAX_MESSAGE_LENGTH_FOR_SUMMARY = 500

    def __init__(self, summarizer_llm: Optional[LLM] = None):
        """Initialize the context compression service.

        Args:
            summarizer_llm: Optional LLM instance for summarization.
                          If not provided, uses default config.
        """
        self.summarizer_llm = summarizer_llm or LLM(config_name="default")

    def should_compress(
        self,
        memory: Memory,
        estimated_tokens: Optional[int] = None,
        max_tokens: Optional[int] = None,
    ) -> bool:
        """Determine if context compression is needed.

        Args:
            memory: Memory instance containing messages
            estimated_tokens: Estimated tokens for the next request
            max_tokens: Maximum allowed tokens

        Returns:
            True if compression should be performed, False otherwise
        """
        if not memory.enable_compression:
            return False

        compressible = memory.get_compressible_messages()
        if len(compressible) < self.MIN_MESSAGES_TO_COMPRESS:
            return False

        # Check based on token usage ratio
        if estimated_tokens and max_tokens:
            # This would need access to current token count, which is in LLM
            # For now, we'll use message count as a proxy
            if len(compressible) > self.MAX_MESSAGES_BEFORE_COMPRESS:
                return True
        elif len(compressible) > self.MAX_MESSAGES_BEFORE_COMPRESS:
            return True

        return False

    def should_compress_by_tokens(
        self,
        memory: Memory,
        current_tokens: int,
        estimated_tokens: int,
        max_tokens: int,
    ) -> bool:
        """Determine if compression is needed based on token usage.

        Args:
            memory: Memory instance containing messages
            current_tokens: Current token count
            estimated_tokens: Estimated tokens for the next request
            max_tokens: Maximum allowed tokens

        Returns:
            True if compression should be performed, False otherwise
        """
        if not memory.enable_compression:
            return False

        compressible = memory.get_compressible_messages()
        if len(compressible) < self.MIN_MESSAGES_TO_COMPRESS:
            return False

        usage_ratio = (current_tokens + estimated_tokens) / max_tokens
        return usage_ratio >= memory.compression_threshold

    async def compress(self, memory: Memory) -> bool:
        """Compress the conversation context.

        Args:
            memory: Memory instance to compress

        Returns:
            True if compression was successful, False otherwise
        """
        compressible = memory.get_compressible_messages()

        if len(compressible) < self.MIN_MESSAGES_TO_COMPRESS:
            return False

        try:
            logger.info(
                f"ðŸ”„ Compressing context: {len(compressible)} messages to summarize "
                f"(keeping {memory.keep_recent_messages} recent messages)"
            )

            # Create summary
            summary_msg = await self._create_summary(compressible)
            memory.summary_message = summary_msg

            # Rebuild messages: system + summary + recent
            system_messages = memory.get_system_messages()
            recent_messages = memory.get_recent_messages(memory.keep_recent_messages)
            memory.messages = system_messages + [summary_msg] + recent_messages

            logger.info(
                f"âœ… Context compressed: {len(compressible)} messages -> 1 summary, "
                f"total messages: {len(memory.messages)} "
                f"(system: {len(system_messages)}, recent: {len(recent_messages)})"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to compress context: {e}", exc_info=True)
            # Fall back to simple truncation
            return self._fallback_truncation(memory)

    async def _create_summary(self, messages: List[Message]) -> Message:
        """Create a summary of the given messages.

        Args:
            messages: List of messages to summarize

        Returns:
            Message containing the summary
        """
        summary_prompt = self._build_summary_prompt(messages)

        summary_response = await self.summarizer_llm.ask(
            messages=[Message.user_message(summary_prompt)],
            system_msgs=[
                Message.system_message(
                    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æå–å’Œä¿ç•™å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œ"
                    "åŒ…æ‹¬ç”¨æˆ·æ„å›¾ã€æ‰§è¡Œçš„æ“ä½œã€é‡è¦ç»“æžœå’Œå½“å‰çŠ¶æ€ã€‚æ€»ç»“è¦ç®€æ´ä½†å®Œæ•´ã€‚"
                )
            ],
            stream=False,
        )

        summary_content = (
            f"[å·²åŽ‹ç¼©çš„å¯¹è¯åŽ†å²æ‘˜è¦ ({len(messages)} æ¡æ¶ˆæ¯)]\n{summary_response}"
        )
        return Message.assistant_message(summary_content)

    def _build_summary_prompt(self, messages: List[Message]) -> str:
        """Build the prompt for summarization.

        Args:
            messages: List of messages to summarize

        Returns:
            Formatted summary prompt
        """
        prompt = """è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯åŽ†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œå†³ç­–ç‚¹ã€‚æ€»ç»“åº”è¯¥ç®€æ´ä½†åŒ…å«é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. ç”¨æˆ·çš„ä¸»è¦éœ€æ±‚å’Œç›®æ ‡
2. å·²æ‰§è¡Œçš„å…³é”®æ“ä½œå’Œå·¥å…·è°ƒç”¨
3. é‡è¦çš„ä¸­é—´ç»“æžœå’Œå‘çŽ°
4. å½“å‰çš„ä»»åŠ¡çŠ¶æ€

å¯¹è¯åŽ†å²ï¼š
"""
        role_names = {"user": "ç”¨æˆ·", "assistant": "åŠ©æ‰‹", "tool": "å·¥å…·"}

        for i, msg in enumerate(messages, 1):
            role_name = role_names.get(msg.role, msg.role)
            content = msg.content or ""

            # Add tool call information
            if msg.tool_calls:
                tool_names = [tc.function.name for tc in msg.tool_calls]
                content += f" [è°ƒç”¨äº†å·¥å…·: {', '.join(tool_names)}]"

            # Limit message length to avoid token explosion
            if len(content) > self.MAX_MESSAGE_LENGTH_FOR_SUMMARY:
                content = content[: self.MAX_MESSAGE_LENGTH_FOR_SUMMARY] + "..."

            prompt += f"\n{i}. {role_name}: {content}"

        prompt += "\n\nè¯·æä¾›ä¸€ä¸ªç®€æ´ä½†å®Œæ•´çš„æ€»ç»“ï¼Œä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼š"
        return prompt

    def _fallback_truncation(self, memory: Memory) -> bool:
        """Fallback to simple message truncation if compression fails.

        Args:
            memory: Memory instance to truncate

        Returns:
            True if truncation was performed
        """
        logger.warning("Falling back to simple message truncation")
        system_messages = memory.get_system_messages()
        recent_messages = memory.get_recent_messages(memory.keep_recent_messages)
        memory.messages = system_messages + recent_messages
        return False
