from abc import ABC, abstractmethod
from contextlib import asynccontextmanager
from typing import List, Optional

from pydantic import BaseModel, Field, model_validator

from app.llm import LLM
from app.logger import logger
from app.sandbox.client import SANDBOX_CLIENT
from app.schema import ROLE_TYPE, AgentState, Memory, Message


class BaseAgent(BaseModel, ABC):
    """Abstract base class for managing agent state and execution.

    Provides foundational functionality for state transitions, memory management,
    and a step-based execution loop. Subclasses must implement the `step` method.
    """

    # Core attributes
    name: str = Field(..., description="Unique name of the agent")
    description: Optional[str] = Field(None, description="Optional agent description")

    # Prompts
    system_prompt: Optional[str] = Field(
        None, description="System-level instruction prompt"
    )
    next_step_prompt: Optional[str] = Field(
        None, description="Prompt for determining next action"
    )

    # Dependencies
    llm: LLM = Field(default_factory=LLM, description="Language model instance")
    memory: Memory = Field(default_factory=Memory, description="Agent's memory store")
    state: AgentState = Field(
        default=AgentState.IDLE, description="Current agent state"
    )

    # Execution control
    max_steps: int = Field(default=10, description="Maximum steps before termination")
    current_step: int = Field(default=0, description="Current step in execution")

    duplicate_threshold: int = 2

    class Config:
        arbitrary_types_allowed = True
        extra = "allow"  # Allow extra fields for flexibility in subclasses

    @model_validator(mode="after")
    def initialize_agent(self) -> "BaseAgent":
        """Initialize agent with default settings if not provided."""
        if self.llm is None or not isinstance(self.llm, LLM):
            self.llm = LLM(config_name=self.name.lower())
        if not isinstance(self.memory, Memory):
            self.memory = Memory()
        return self

    @asynccontextmanager
    async def state_context(self, new_state: AgentState):
        """Context manager for safe agent state transitions.

        Args:
            new_state: The state to transition to during the context.

        Yields:
            None: Allows execution within the new state.

        Raises:
            ValueError: If the new_state is invalid.
        """
        if not isinstance(new_state, AgentState):
            raise ValueError(f"Invalid state: {new_state}")

        previous_state = self.state
        self.state = new_state
        try:
            yield
        except Exception as e:
            self.state = AgentState.ERROR  # Transition to ERROR on failure
            raise e
        finally:
            self.state = previous_state  # Revert to previous state

    def update_memory(
        self,
        role: ROLE_TYPE,  # type: ignore
        content: str,
        base64_image: Optional[str] = None,
        **kwargs,
    ) -> None:
        """Add a message to the agent's memory.

        Args:
            role: The role of the message sender (user, system, assistant, tool).
            content: The message content.
            base64_image: Optional base64 encoded image.
            **kwargs: Additional arguments (e.g., tool_call_id for tool messages).

        Raises:
            ValueError: If the role is unsupported.
        """
        message_map = {
            "user": Message.user_message,
            "system": Message.system_message,
            "assistant": Message.assistant_message,
            "tool": lambda content, **kw: Message.tool_message(content, **kw),
        }

        if role not in message_map:
            raise ValueError(f"Unsupported message role: {role}")

        # Create message with appropriate parameters based on role
        kwargs = {"base64_image": base64_image, **(kwargs if role == "tool" else {})}
        self.memory.add_message(message_map[role](content, **kwargs))

    async def run(self, request: Optional[str] = None) -> str:
        """Execute the agent's main loop asynchronously.

        Args:
            request: Optional initial user request to process.

        Returns:
            A string summarizing the execution results.

        Raises:
            RuntimeError: If the agent is not in IDLE state at start.
        """
        if self.state != AgentState.IDLE:
            raise RuntimeError(f"Cannot run agent from state: {self.state}")

        if request:
            self.update_memory("user", request)

        results: List[str] = []
        async with self.state_context(AgentState.RUNNING):
            while (
                self.current_step < self.max_steps and self.state != AgentState.FINISHED
            ):
                self.current_step += 1
                logger.info(f"Executing step {self.current_step}/{self.max_steps}")
                step_result = await self.step()

                # Check for stuck state
                if self.is_stuck():
                    self.handle_stuck_state()

                results.append(f"Step {self.current_step}: {step_result}")

            if self.current_step >= self.max_steps:
                self.current_step = 0
                self.state = AgentState.IDLE
                results.append(f"Terminated: Reached max steps ({self.max_steps})")
        await SANDBOX_CLIENT.cleanup()
        return "\n".join(results) if results else "No steps executed"

    @abstractmethod
    async def step(self) -> str:
        """Execute a single step in the agent's workflow.

        Must be implemented by subclasses to define specific behavior.
        """

    def handle_stuck_state(self):
        """Handle stuck state by adding a prompt to change strategy"""
        stuck_prompt = "\
        Observed duplicate responses. Consider new strategies and avoid repeating ineffective paths already attempted."
        self.next_step_prompt = f"{stuck_prompt}\n{self.next_step_prompt}"
        logger.warning(f"Agent detected stuck state. Added prompt: {stuck_prompt}")

    def is_stuck(self) -> bool:
        """Check if the agent is stuck in a loop by detecting duplicate content"""
        if len(self.memory.messages) < 2:
            return False

        last_message = self.memory.messages[-1]
        if not last_message.content:
            return False

        # Count identical content occurrences
        duplicate_count = sum(
            1
            for msg in reversed(self.memory.messages[:-1])
            if msg.role == "assistant" and msg.content == last_message.content
        )

        return duplicate_count >= self.duplicate_threshold

    async def compress_context_if_needed(
        self, estimated_tokens: Optional[int] = None, max_tokens: Optional[int] = None
    ) -> bool:
        """Compress context if token usage is approaching limits.
        
        Uses a sliding window approach:
        - Keeps all system messages
        - Keeps the most recent N messages (configurable)
        - Summarizes messages in between
        
        Args:
            estimated_tokens: Estimated tokens for the next request
            max_tokens: Maximum allowed tokens
            
        Returns:
            True if compression was performed, False otherwise
        """
        if not self.memory.enable_compression:
            return False

        # Get compressible messages (excluding system and recent)
        compressible = self.memory.get_compressible_messages()
        if len(compressible) < 5:  # Don't compress if too few messages
            return False

        # Check if compression is needed based on token usage
        should_compress = False
        if estimated_tokens and max_tokens:
            usage_ratio = (self.llm.total_input_tokens + estimated_tokens) / max_tokens
            if usage_ratio >= self.memory.compression_threshold:
                should_compress = True
        elif len(compressible) > 20:  # Compress if too many messages
            should_compress = True

        if not should_compress:
            return False

        try:
            logger.info(
                f"ðŸ”„ Compressing context: {len(compressible)} messages to summarize "
                f"(keeping {self.memory.keep_recent_messages} recent messages)"
            )
            
            # Prepare messages for summarization
            messages_to_summarize = compressible
            
            # Create summary prompt
            summary_prompt = """è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯åŽ†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œå†³ç­–ç‚¹ã€‚æ€»ç»“åº”è¯¥ç®€æ´ä½†åŒ…å«é‡è¦çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬ï¼š
1. ç”¨æˆ·çš„ä¸»è¦éœ€æ±‚å’Œç›®æ ‡
2. å·²æ‰§è¡Œçš„å…³é”®æ“ä½œå’Œå·¥å…·è°ƒç”¨
3. é‡è¦çš„ä¸­é—´ç»“æžœå’Œå‘çŽ°
4. å½“å‰çš„ä»»åŠ¡çŠ¶æ€

å¯¹è¯åŽ†å²ï¼š
"""
            for i, msg in enumerate(messages_to_summarize, 1):
                role_name = {"user": "ç”¨æˆ·", "assistant": "åŠ©æ‰‹", "tool": "å·¥å…·"}.get(msg.role, msg.role)
                content = msg.content or ""
                if msg.tool_calls:
                    tool_names = [tc.function.name for tc in msg.tool_calls]
                    content += f" [è°ƒç”¨äº†å·¥å…·: {', '.join(tool_names)}]"
                # Limit each message to 500 chars to avoid token explosion
                if len(content) > 500:
                    content = content[:500] + "..."
                summary_prompt += f"\n{i}. {role_name}: {content}"
            
            summary_prompt += "\n\nè¯·æä¾›ä¸€ä¸ªç®€æ´ä½†å®Œæ•´çš„æ€»ç»“ï¼Œä¿ç•™æ‰€æœ‰å…³é”®ä¿¡æ¯ï¼š"

            # Generate summary using LLM (use a separate LLM instance to avoid recursion)
            # Use default config to avoid token limit issues
            summary_llm = LLM(config_name="default")
            summary_response = await summary_llm.ask(
                messages=[Message.user_message(summary_prompt)],
                system_msgs=[
                    Message.system_message(
                        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¯¹è¯æ€»ç»“åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯æå–å’Œä¿ç•™å¯¹è¯ä¸­çš„å…³é”®ä¿¡æ¯ï¼Œ"
                        "åŒ…æ‹¬ç”¨æˆ·æ„å›¾ã€æ‰§è¡Œçš„æ“ä½œã€é‡è¦ç»“æžœå’Œå½“å‰çŠ¶æ€ã€‚æ€»ç»“è¦ç®€æ´ä½†å®Œæ•´ã€‚"
                    )
                ],
                stream=False,
            )

            # Create summary message
            summary_content = f"[å·²åŽ‹ç¼©çš„å¯¹è¯åŽ†å²æ‘˜è¦ ({len(messages_to_summarize)} æ¡æ¶ˆæ¯)]\n{summary_response}"
            summary_msg = Message.assistant_message(summary_content)
            self.memory.summary_message = summary_msg

            # Replace compressible messages with summary
            system_messages = self.memory.get_system_messages()
            recent_messages = self.memory.get_recent_messages(self.memory.keep_recent_messages)
            
            # Rebuild messages: system + summary + recent
            self.memory.messages = system_messages + [summary_msg] + recent_messages
            
            logger.info(
                f"âœ… Context compressed: {len(messages_to_summarize)} messages -> 1 summary, "
                f"total messages: {len(self.memory.messages)} "
                f"(system: {len(system_messages)}, recent: {len(recent_messages)})"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ Failed to compress context: {e}", exc_info=True)
            # If compression fails, fall back to simple truncation
            logger.warning("Falling back to simple message truncation")
            system_messages = self.memory.get_system_messages()
            recent_messages = self.memory.get_recent_messages(self.memory.keep_recent_messages)
            self.memory.messages = system_messages + recent_messages
            return False

    @property
    def messages(self) -> List[Message]:
        """Retrieve a list of messages from the agent's memory."""
        return self.memory.messages

    @messages.setter
    def messages(self, value: List[Message]):
        """Set the list of messages in the agent's memory."""
        self.memory.messages = value
